# -*- coding: utf-8 -*-
"""EnKF.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BLn2EknqH64vykf_wN8oFxqiGijlZ0FB
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.integrate import solve_ivp

# Constants
g = 9.81  # Gravity (m/s^2)
theta = np.radians(45)  # Launch angle in radians
v0 = 20  # Initial speed (m/s)
T = 5  # Simulation time (s)
dt = 0.5  # Time step

# Function defining ODEs for projectile motion
def projectile_motion(t, state):
    x, y, vx, vy = state  # Unpacking state variables
    dx_dt = vx
    dy_dt = vy
    dvx_dt = 0  # No horizontal acceleration
    dvy_dt = -g  # Acceleration due to gravity
    return [dx_dt, dy_dt, dvx_dt, dvy_dt]

# Initial conditions: (x0, y0, vx0, vy0)
x0 = 0
y0 = 0
vx0 = v0 * np.cos(theta)
vy0 = v0 * np.sin(theta)
initial_state = [x0, y0, vx0, vy0]

# Time array
t = np.arange(0, T, dt)

# Solving the ODEs using solve_ivp
solution = solve_ivp(projectile_motion, [0, T], initial_state, t_eval=t, method='RK45')

# Extracting solutions
x_sol = solution.y[0]
y_sol = solution.y[1]

# Plot trajectory
plt.plot(x_sol, y_sol)
plt.xlabel("Horizontal Distance (m)")
plt.ylabel("Vertical Distance (m)")
plt.title("Projectile Motion")
plt.grid()
plt.show()

#generate synthetic observations
noise_std = 2 #this might be kind've high, we'll test with other vals

true_horizontal = x_sol
true_vertical = y_sol

horizontal_observations = true_horizontal + np.random.normal(0, noise_std, len(true_horizontal))
vertical_observations = true_vertical + np.random.normal(0, noise_std, len(true_vertical))

plt.plot(x_sol, y_sol)
plt.scatter(horizontal_observations, vertical_observations, color='red', label='Observations')
plt.xlabel("Horizontal Distance (m)")
plt.ylabel("Vertical Distance (m)")
plt.title("Projectile Motion with Observations")
plt.legend()
plt.grid()
plt.show()

#define some helper functions

#get ensemble mean
def get_ensemble_mean(X, N):
  mean = X[0]
  for i in range(1,N):
    ensemble = X[i]
    mean += ensemble
  return mean * (1/N)

#get perturbation matrix
def get_perturbation_matrix(N, X_mean, X):
  perturbation_matrix = np.zeros((4,N))
  for i in range(N):
    perturbation_matrix[:,i] = np.subtract(X[i], X_mean)
  return perturbation_matrix.T

#covar matrix
def get_covar_matrix(N, perturbation):
  covar_matrix = np.zeros((4,4))
  for i in range(N):
    covar_matrix = np.add(covar_matrix, (np.outer(perturbation[i], perturbation[i].T)))
  covar_matrix = covar_matrix * (1 / N)
  return covar_matrix

def distnace(x1, y1, x2, y2):
  return np.sqrt((x2 - x1)**2 + (y2 - y1)**2)

def plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, timestep):
  plt.plot(x_sol, y_sol)
  plt.scatter(horizontal_observations[timestep], vertical_observations[timestep], color='red', label='observed')
  plt.scatter(x_sol[timestep], y_sol[timestep], color='green', label='true')
  plt.scatter(x_means[timestep], y_means[timestep], edgecolors= 'black', color='orange', label='ensemble mean')
  plt.scatter(x_ensembles[timestep], y_ensembles[timestep], alpha=0.1, color='blue', label='ensemble members')
  plt.xlabel("Horizontal Distance (m)")
  plt.ylabel("Vertical Distance (m)")
  plt.title(f"EnKF performance at iteration {timestep}")
  plt.legend()
  plt.grid()
  plt.show()

#initialize conditions
#change Perturbation, covar, F, and Q to match system
#I'm testing the size of each matrix with a print which i'll leave a comment of next to
#hoping this will let us see mistakes and errors faster!
N = 10 #Ensemble size
X = np.zeros((4, N)).T #Ensemble matrix (10x4)
X_mean = get_ensemble_mean(X, N) #Ensemble mean (1x4)
ensemble_perturbation = get_perturbation_matrix(N, X_mean, X) #Perturbation matrix (10x4)
covar_matrix = get_covar_matrix(N, ensemble_perturbation) #covariance matrix (20x20)
Q = [[0.01, 0, 0, 0], [0, 0.01, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]] #error (4x4)
observation_matrix = np.array([horizontal_observations, vertical_observations]).T

#EnKF algorithm
#TODO: debug and finish analysis step than make corrections, visualize and interpret
#I think we can finish the code by next weeks meeting and try using with other data/contexts
#we get our mean, each ensemble is preturbed version of mean each iteration
def EnKF_solve(N, X0, covar_matrix0, F, H, Q, G, observation_matrix):
  iterations = len(observation_matrix)
  u_n = [0,1] #control vector
  x = X0.copy()
  xf_all = x.copy()
  return_matrix_x = np.zeros((iterations, N))
  return_matrix_y = np.zeros((iterations, N))
  x_means = np.zeros(iterations)
  y_means = np.zeros(iterations)
  for i in range(iterations):
    #forecast step
    for j in range(N): #updating each ensemble
      xa = x.copy()
      xa[j] = xa[j] + np.random.randn(1, 4)
      xf = F @ xa[j] + G @ u_n
      xf_all[j] = xf
    return_matrix_x[i] = xf_all.T[0].copy()
    return_matrix_y[i] = xf_all.T[1].copy()
    x_mean = get_ensemble_mean(xf_all, N)
    ensemble_perturbation = get_perturbation_matrix(N, x_mean, xf_all)
    Pf = F @ get_covar_matrix(N, ensemble_perturbation) @ F.T + Q
    #Analysis step
    S = H @ Pf @ H.T# + Q
    K = Pf @ H.T @ np.linalg.inv(S) #Kalman gain
    d = observation_matrix[i] - H @ x_mean
    x[:] = x_mean + K @ d
    P = (np.eye(len(x[i])) - K @ H) @ Pf
    x_means[i] = x_mean[0]
    y_means[i] = x_mean[1]
    #ToDo: create arrays to return and return
  return return_matrix_x, return_matrix_y, x_means, y_means

#EnKF test and visuals
#ToDO, intialize the F matrix and visualize
F = np.array([[1, 0, dt, 0], [0, 1, 0, dt], [0, 0, 1, 0], [0, 0, 0, 1]]) #based off odes
G = np.array([[0, 0], [0, 0], [0, 0], [0, -g * dt]]) #control matrix
H = np.array([[1, 0, 0, 0], [0, 1, 0, 0]]) #make into 2x4
x_ensembles, y_ensembles, x_means, y_means = EnKF_solve(N, X, covar_matrix, F, H, Q, G, observation_matrix)
error = distnace(x_means, y_means, x_sol, y_sol)
size = T / dt
xs = np.arange(size)

plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 0)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 1)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 2)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 3)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 4)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 5)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 6)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 7)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 8)
plot_ensemble(x_means, y_means, x_sol, y_sol, x_ensembles, y_ensembles, horizontal_observations, vertical_observations, 9)

plt.scatter(xs, error, color='red', label='error')
plt.xlabel("Timestep")
plt.ylabel("Error")
plt.title(f"Error between EnKF computed mean and solution")
plt.legend()
plt.grid()
plt.show()

#journal:
'''as of now, our model seems to do this funny thing where it falls behind and doesn't work particularly well.
trying to diagnose problem, maybe need to add a G matrix to account gravity? y's fall behind way more than x's

update: made a control matrix and implemented it. Model seems to be a time step ahead for some reason,
also y-position isn't peaking nearly as high as it should. Will continue to attempt to fix

another update: changed where the -g is on the control matrix. Our model works kind've in the middle,
it's initialization might have some problems based on the first few iterations, and seems to fall behind
towards the end. Something I find interesting is the way the ensembles form a perfect linear line,
I wanna try to figure out what's causing that. might be the way random.randn works

Ok it just got a lot better, i modified control matrix again. for some reason the second iteration also does
something crazy, but for at least the position we're tracking it pretty well for the most part

Things to fix as of now are figuring out why the second iteration always does something werid and i don't necessarily
know that it's a problem but experimenting with a different random function to see if we can get ensembles that don't
form a perfect line

The model starts off weird but it does end up making good predictions after a few iterations, so I'm thinking
despite it's flaws it actually achomplishes the goal

I didn't give random.randn a matrix size lol'''